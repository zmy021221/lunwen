{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a10e5636-bc03-4391-a7b3-46177e3b6bb5",
   "metadata": {},
   "source": [
    "环境配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8a95ba-8e5e-46f5-83b0-a422bfc9c39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from arch import arch_model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import time\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7926774d-c613-4b8d-aae2-96b673de09e6",
   "metadata": {},
   "source": [
    "数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeac648-5871-4ed5-87c8-0d32e2cca23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "data = pd.read_csv('AAPL历史数据.csv', parse_dates=['日期'])\n",
    "data.set_index('日期', inplace=True)\n",
    "close_prices = data[['收盘']].values\n",
    "\n",
    "# 归一化\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))  \n",
    "scaled_data = scaler.fit_transform(close_prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47db9abf-a0e3-4832-bdb7-f3351cfb60a4",
   "metadata": {},
   "source": [
    "生成合成数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384a5c8f-a0a3-47ee-a88d-9fc1baf8348f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 自回归模型（AR）\n",
    "ar_model = AutoReg(scaled_data, lags=5).fit()\n",
    "ar_forecast = ar_model.predict(start=len(scaled_data), end=len(scaled_data)+100, dynamic=False)\n",
    "\n",
    "# 广义自回归条件异方差模型（GARCH）\n",
    "scale_factor = 10  \n",
    "scaled_data_for_garch = scaled_data * scale_factor\n",
    "\n",
    "garch_model = arch_model(scaled_data_for_garch, vol='GARCH', p=1, q=1)\n",
    "garch_results = garch_model.fit(disp='off')\n",
    "garch_forecast_scaled = garch_results.forecast(horizon=100)\n",
    "garch_forecast = garch_forecast_scaled.variance.iloc[-1].values / (scale_factor ** 2)\n",
    "\n",
    "# 分数布朗运动（fBm）\n",
    "def fbm(n, H):\n",
    "    t = np.linspace(0, 1, n)\n",
    "    W = np.random.normal(size=n)\n",
    "    W = np.cumsum(W) * (1.0 / n)**H\n",
    "    return W\n",
    "\n",
    "fbm_data = fbm(100, 0.7)\n",
    "\n",
    "# 输出结果\n",
    "print(\"AR模型预测结果:\", ar_forecast)\n",
    "print(\"GARCH模型预测结果:\", garch_forecast)\n",
    "print(\"分数布朗运动数据:\", fbm_data)\n",
    "\n",
    "# 可视化分析\n",
    "\n",
    "# AR模型可视化\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ar_forecast, label='AR模型预测结果', color='blue', marker='o', linestyle='-', linewidth=1)\n",
    "plt.title('')\n",
    "plt.xlabel('时间步长')\n",
    "plt.ylabel('预测值')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('AR模型预测结果.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# GARCH模型可视化\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(garch_forecast, label='GARCH模型预测波动率', color='green', marker='x', linestyle='--', linewidth=1)\n",
    "plt.title('')\n",
    "plt.xlabel('时间步长')\n",
    "plt.ylabel('波动率')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('GARCH模型预测波动率.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# 分数布朗运动可视化\n",
    "plt.figure(figsize=(10, 6))\n",
    "for _ in range(5):  # 绘制5条路径进行展示\n",
    "    fbm_path = fbm(100, 0.7)\n",
    "    plt.plot(fbm_path, label=f'fBm路径 {_+1}', alpha=0.7)\n",
    "plt.title('')\n",
    "plt.xlabel('时间步长')\n",
    "plt.ylabel('值')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('分数布朗运动路径图.png', dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea214ccb-5ee4-4621-b3fd-c51868e3b4a6",
   "metadata": {},
   "source": [
    "SBTS模型的实现与训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaa4674-cb4b-43de-85b5-77e30986681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 假设 scaled_data 已经被定义并预处理\n",
    "# 假设 data 是原始的 pandas DataFrame\n",
    "\n",
    "# 创建增强数据集\n",
    "sequence_length = 60\n",
    "X, y = [], []\n",
    "for i in range(len(scaled_data) - sequence_length):\n",
    "    X.append(scaled_data[i:i+sequence_length])\n",
    "    y.append(scaled_data[i+sequence_length])\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "# 划分训练集和验证集\n",
    "train_ratio = 0.8\n",
    "train_size = int(len(dataset) * train_ratio)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# 使用指定的超参数\n",
    "best_params = {\n",
    "    'hidden_size': 512,\n",
    "    'learning_rate': 0.0005,\n",
    "    'num_layers': 4,\n",
    "    'dropout_rate': 0.2\n",
    "}\n",
    "\n",
    "# 定义模型\n",
    "class EnhancedSBTS(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_rate):\n",
    "        super(EnhancedSBTS, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim=hidden_size*2, num_heads=8)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size*2, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(64, input_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out.permute(1, 0, 2)\n",
    "        attn_out, _ = self.self_attn(out, out, out)\n",
    "        out = attn_out.permute(1, 0, 2)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out.squeeze(-1)\n",
    "\n",
    "# 使用指定超参数初始化模型\n",
    "model = EnhancedSBTS(input_size=1, hidden_size=best_params['hidden_size'], num_layers=best_params['num_layers'], dropout_rate=best_params['dropout_rate'])\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=best_params['learning_rate'], weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "\n",
    "# 注释掉训练部分的代码，但保留训练时间变量\n",
    "# train_dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "# \n",
    "# model.train()\n",
    "# start_time = time.time()\n",
    "# for epoch in range(50):  # 修改训练轮次为 50\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     for batch_idx, (X_batch, y_batch) in enumerate(train_dataloader):\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(X_batch)\n",
    "#         loss = criterion(outputs, y_batch.squeeze(-1))\n",
    "#         loss.backward()\n",
    "#         nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "#         progress = (batch_idx + 1) / len(train_dataloader) * 100\n",
    "#         print(f'Epoch {epoch+1}/50, Batch {batch_idx+1}/{len(train_dataloader)}, Progress: {progress:.2f}%, Loss: {loss.item():.6f}', end='\\r')\n",
    "#     avg_loss = total_loss / len(train_dataloader)\n",
    "#     scheduler.step(avg_loss)\n",
    "#     print(f'\\nEpoch {epoch+1}, Average Loss: {avg_loss:.6f}')\n",
    "# train_time_sbts = time.time() - start_time\n",
    "\n",
    "# 初始化生成时间变量\n",
    "gen_time_sbts = 0\n",
    "train_time_sbts = 0\n",
    "\n",
    "# 生成增强数据\n",
    "model.eval()\n",
    "start_time = time.time()  # 开始计时\n",
    "with torch.no_grad():\n",
    "    generated = []\n",
    "    seed_sequence = X_tensor[0:1]\n",
    "    \n",
    "    # 应用核密度估计增强数据\n",
    "    def kernel_density_estimation(data, h=0.05):\n",
    "        return torch.normal(mean=data, std=h)\n",
    "    \n",
    "    for _ in range(len(X_tensor)):\n",
    "        # 生成预测\n",
    "        pred = model(seed_sequence)\n",
    "        \n",
    "        # 应用核密度估计来增强数据，生成1个样本\n",
    "        enhanced_pred = kernel_density_estimation(pred, h=0.05)\n",
    "        generated.append(enhanced_pred.numpy()[0])\n",
    "        \n",
    "        # 更新序列以继续生成\n",
    "        pred_reshaped = pred.unsqueeze(1).unsqueeze(2)\n",
    "        new_seq = torch.cat([seed_sequence[:, 1:, :], pred_reshaped], dim=1)\n",
    "        seed_sequence = new_seq\n",
    "    \n",
    "    generated_data_sbts = np.array(generated)\n",
    "gen_time_sbts = time.time() - start_time  # 结束计时并记录生成时间\n",
    "\n",
    "# 使用正确的逆变换方法\n",
    "generated_data_sbts = scaler.inverse_transform(generated_data_sbts.reshape(-1, 1))\n",
    "\n",
    "# 对齐数据\n",
    "synth_df = pd.DataFrame(\n",
    "    generated_data_sbts,\n",
    "    index=data.index[sequence_length:sequence_length + len(generated_data_sbts)],\n",
    "    columns=['收盘']  # 假设列名为“收盘”\n",
    ")\n",
    "\n",
    "# 确保 real_data 和 synth_data 的长度一致\n",
    "real_data = data.iloc[sequence_length:sequence_length + len(synth_df)]\n",
    "\n",
    "# 检查数据长度是否一致\n",
    "print(f\"生成数据长度: {len(synth_df)}\")\n",
    "print(f\"原始数据长度: {len(real_data)}\")\n",
    "\n",
    "# 绘制结果\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(real_data.index, real_data['收盘'], label='真实数据', color='steelblue')\n",
    "plt.plot(synth_df.index, synth_df['收盘'], label='合成数据', color='darkorange', alpha=0.7)\n",
    "plt.xlabel('日期')\n",
    "plt.ylabel('收盘价')\n",
    "plt.title('真实数据与合成数据对比')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 输出时间和数据变量\n",
    "print(f\"训练时间: {train_time_sbts} 秒\")\n",
    "print(f\"生成时间: {gen_time_sbts} 秒\")\n",
    "print(f\"生成的数据已存储到 generated_data_sbts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690b9ba4-f69c-47c6-bb7e-5b6196f1c5f7",
   "metadata": {},
   "source": [
    "TimeGAN模型的实现与训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68955bb5-1823-4cee-94d3-c2b02779a4d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()  # 添加 sigmoid 激活函数\n",
    "   \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(1, x.size(0), hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out)\n",
    "        out = self.sigmoid(out)  \n",
    "        return out\n",
    "        \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()  # 添加 sigmoid 激活函数\n",
    "   \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(1, x.size(0), hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        out = self.sigmoid(out)  \n",
    "        return out\n",
    "\n",
    "# 参数设置\n",
    "input_size = 1\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "\n",
    "# 初始化模型和优化器\n",
    "generator = Generator(input_size, hidden_size, output_size)\n",
    "discriminator = Discriminator(input_size, hidden_size, 1)\n",
    "optimizer_g = torch.optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 训练模型\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        # 训练判别器\n",
    "        optimizer_d.zero_grad()\n",
    "        real_outputs = discriminator(X_batch)\n",
    "        real_labels = torch.ones(real_outputs.size(0), 1).to(X_batch.device)\n",
    "        fake_data = generator(X_batch)\n",
    "        fake_outputs = discriminator(fake_data.detach())\n",
    "        fake_labels = torch.zeros(fake_outputs.size(0), 1).to(X_batch.device)\n",
    "        d_loss = criterion(real_outputs, real_labels) + criterion(fake_outputs, fake_labels)\n",
    "        d_loss.backward()\n",
    "        optimizer_d.step()\n",
    "        \n",
    "        # 训练生成器\n",
    "        optimizer_g.zero_grad()\n",
    "        fake_data = generator(X_batch)\n",
    "        g_outputs = discriminator(fake_data)\n",
    "        g_loss = criterion(g_outputs, real_labels)\n",
    "        g_loss.backward()\n",
    "        optimizer_g.step()\n",
    "train_time_timegan = time.time() - start_time  # 存储训练时间到 train_time_timegan\n",
    "\n",
    "# 生成数据\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    generated_data_timegan = generator(X_tensor)  # 存储生成的数据到 generated_data_timegan\n",
    "gen_time_timegan = time.time() - start_time  # 存储生成时间到 gen_time_timegan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e5be3c-0662-46c2-bec8-550dcd34319e",
   "metadata": {},
   "source": [
    "Diffusion-SB模型的实现与训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569b8e80-06c1-4965-84e8-47054d8e9638",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionSB(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, T=100):\n",
    "        super(DiffusionSB, self).__init__()\n",
    "        self.T = T\n",
    "        self.beta = torch.linspace(0.0001, 0.02, T)\n",
    "        self.alpha = 1 - self.beta\n",
    "        self.alpha_bar = torch.cumprod(self.alpha, dim=0)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        h0 = torch.zeros(1, x.size(0), hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(1, x.size(0), hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# 参数设置\n",
    "input_size = 1\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "T = 100\n",
    "\n",
    "# 初始化模型和优化器\n",
    "model = DiffusionSB(input_size, hidden_size, output_size, T)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 训练模型\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        t = torch.randint(0, T, (X_batch.size(0),)).to(X_batch.device)\n",
    "        noise = torch.randn_like(X_batch)\n",
    "        alpha_bar_t = model.alpha_bar[t].view(-1, 1, 1)\n",
    "        x_t = alpha_bar_t.sqrt() * X_batch + (1 - alpha_bar_t).sqrt() * noise\n",
    "        predicted_noise = model(x_t, t)\n",
    "        loss = nn.MSELoss()(predicted_noise, noise)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "train_time_diffusion = time.time() - start_time  # 存储训练时间到 train_time_diffusion\n",
    "\n",
    "# 生成数据\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    generated_data_diffusion = torch.randn(X_tensor.size(0), 100, 1).to(X_tensor.device)\n",
    "    for t in reversed(range(T)):\n",
    "        alpha_t = model.alpha[t]\n",
    "        alpha_bar_t = model.alpha_bar[t]\n",
    "        beta_t = model.beta[t]\n",
    "        predicted_noise = model(generated_data_diffusion, torch.full((generated_data_diffusion.size(0),), t, dtype=torch.long).to(generated_data_diffusion.device))\n",
    "        generated_data_diffusion = (generated_data_diffusion - beta_t / (1 - alpha_bar_t).sqrt() * predicted_noise) / alpha_t.sqrt()\n",
    "        if t > 0:\n",
    "            generated_data_diffusion += model.beta[t].sqrt() * torch.randn_like(generated_data_diffusion)\n",
    "gen_time_diffusion = time.time() - start_time  # 存储生成时间到 gen_time_diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1235e62-ec58-4fc8-8800-022bf806eb6a",
   "metadata": {},
   "source": [
    "CAVE模型的实现与训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2741c9e2-6b87-4a77-bc4a-ce8194aada8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, latent_size):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.encoder = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc_mu = nn.Linear(hidden_size, latent_size)\n",
    "        self.fc_logvar = nn.Linear(hidden_size, latent_size)\n",
    "        self.decoder = nn.LSTM(latent_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, input_size)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(1, x.size(0), hidden_size).to(x.device)\n",
    "        out, _ = self.encoder(x, (h0, c0))\n",
    "        mu = self.fc_mu(out[:, -1, :])\n",
    "        logvar = self.fc_logvar(out[:, -1, :])\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        h0 = torch.zeros(1, z.size(0), hidden_size).to(z.device)\n",
    "        c0 = torch.zeros(1, z.size(0), hidden_size).to(z.device)\n",
    "        z = z.unsqueeze(1).repeat(1, 100, 1)\n",
    "        out, _ = self.decoder(z, (h0, c0))\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        reconstructed = self.decode(z)\n",
    "        return reconstructed, mu, logvar\n",
    "\n",
    "# 参数设置\n",
    "input_size = 1\n",
    "hidden_size = 64\n",
    "latent_size = 16\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "\n",
    "# 初始化模型和优化器\n",
    "model = CVAE(input_size, hidden_size, latent_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 训练模型\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        reconstructed, mu, logvar = model(X_batch)\n",
    "        reconstruction_loss = nn.MSELoss()(reconstructed, X_batch)\n",
    "        kl_loss = 0.5 * torch.sum(torch.exp(logvar) + mu**2 - 1 - logvar)\n",
    "        loss = reconstruction_loss + kl_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "train_time_cvae = time.time() - start_time  # 存储训练时间到 train_time_cvae\n",
    "\n",
    "# 生成数据\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(X_tensor.size(0), latent_size).to(X_tensor.device)\n",
    "    generated_data_cvae = model.decode(z)  # 存储生成的数据到 generated_data_cvae\n",
    "gen_time_cvae = time.time() - start_time  # 存储生成时间到 gen_time_cvae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a032353f-455a-4e7b-a20b-4522d7247c87",
   "metadata": {},
   "source": [
    "统计特性分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e2591e-d712-4a0f-9f1f-cc4c11e4cd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_statistics(data):\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    skew = pd.Series(data.flatten()).skew()\n",
    "    kurtosis = pd.Series(data.flatten()).kurtosis()\n",
    "    return mean, std, skew, kurtosis\n",
    "\n",
    "# 计算统计特性\n",
    "real_mean, real_std, real_skew, real_kurtosis = calculate_statistics(scaled_data)\n",
    "sbts_mean, sbts_std, sbts_skew, sbts_kurtosis = calculate_statistics(generated_data_sbts.numpy())\n",
    "timegan_mean, timegan_std, timegan_skew, timegan_kurtosis = calculate_statistics(generated_data_timegan.numpy())\n",
    "diffusion_mean, diffusion_std, diffusion_skew, diffusion_kurtosis = calculate_statistics(generated_data_diffusion.numpy())\n",
    "cvae_mean, cvae_std, cvae_skew, cvae_kurtosis = calculate_statistics(generated_data_cvae.numpy())\n",
    "\n",
    "# 打印结果\n",
    "print(\"真实数据: 均值=%.4f, 标准差=%.4f, 偏度=%.4f, 峰度=%.4f\" % (real_mean, real_std, real_skew, real_kurtosis))\n",
    "print(\"SBTS: 均值=%.4f, 标准差=%.4f, 偏度=%.4f, 峰度=%.4f\" % (sbts_mean, sbts_std, sbts_skew, sbts_kurtosis))\n",
    "print(\"TimeGAN: 均值=%.4f, 标准差=%.4f, 偏度=%.4f, 峰度=%.4f\" % (timegan_mean, timegan_std, timegan_skew, timegan_kurtosis))\n",
    "print(\"Diffusion-SB: 均值=%.4f, 标准差=%.4f, 偏度=%.4f, 峰度=%.4f\" % (diffusion_mean, diffusion_std, diffusion_skew, diffusion_kurtosis))\n",
    "print(\"CVAE: 均值=%.4f, 标准差=%.4f, 偏度=%.4f, 峰度=%.4f\" % (cvae_mean, cvae_std, cvae_skew, cvae_kurtosis))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49e1b9f-2b3c-469c-a76c-59a1d5a99be7",
   "metadata": {},
   "source": [
    "ACF和PACF图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cb3e2a-6c65-4bfc-a910-a76f61ba5064",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "\n",
    "# 绘制真实数据的ACF\n",
    "plt.figure(figsize=(6, 4))\n",
    "plot_acf(data['收盘'].values)\n",
    "plt.title('')\n",
    "plt.tight_layout()\n",
    "plt.savefig('real_acf.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# 绘制真实数据的PACF\n",
    "plt.figure(figsize=(6, 4))\n",
    "plot_pacf(data['收盘'].values)\n",
    "plt.title('')\n",
    "plt.tight_layout()\n",
    "plt.savefig('real_pacf.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# 绘制SBTS生成数据的ACF\n",
    "plt.figure(figsize=(6, 4))\n",
    "plot_acf(generated_data_sbts[:, -1].numpy())\n",
    "plt.title('')\n",
    "plt.tight_layout()\n",
    "plt.savefig('sbts_acf.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# 绘制SBTS生成数据的PACF\n",
    "plt.figure(figsize=(6, 4))\n",
    "plot_pacf(generated_data_sbts[:, -1].numpy())\n",
    "plt.title('')\n",
    "plt.tight_layout()\n",
    "plt.savefig('sbts_pacf.png', dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476b1b7d-43ea-4377-9881-c5771999ce93",
   "metadata": {},
   "source": [
    "训练和生成时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375cb85f-f39d-43fe-b6ae-7a875496d2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SBTS 训练时间: %.2f秒, 生成时间: %.2f秒\" % (train_time_sbts, gen_time_sbts))\n",
    "print(\"TimeGAN 训练时间: %.2f秒, 生成时间: %.2f秒\" % (train_time_timegan, gen_time_timegan))\n",
    "print(\"Diffusion-SB 训练时间: %.2f秒, 生成时间: %.2f秒\" % (train_time_diffusion, gen_time_diffusion))\n",
    "print(\"CVAE 训练时间: %.2f秒, 生成时间: %.2f秒\" % (train_time_cvae, gen_time_cvae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3ddbb1-214e-41c9-990b-26cbbfcffd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 对齐数据\n",
    "synth_df = pd.DataFrame(\n",
    "    generated_data_sbts, \n",
    "    index=data.index[sequence_length:sequence_length + len(generated_data_sbts)],\n",
    "    columns=['收盘']\n",
    ")\n",
    "\n",
    "# 确保 real_data 和 synth_data 的长度一致\n",
    "real_data = data.iloc[sequence_length:sequence_length + len(synth_df)]\n",
    "synth_data = synth_df.copy()  \n",
    "\n",
    "# 仅选择2024年7月后的数据\n",
    "start_date = '2024-07-01'  # 2024年7月开始的日期\n",
    "real_data_2024 = real_data.loc[real_data.index >= start_date]\n",
    "synth_data_2024 = synth_data.loc[synth_data.index >= start_date]\n",
    "\n",
    "# 截取相同长度的数据\n",
    "min_length = min(len(real_data_2024), len(synth_data_2024))\n",
    "real_data_2024 = real_data_2024.iloc[:min_length]\n",
    "synth_data_2024 = synth_data_2024.iloc[:min_length]\n",
    "\n",
    "# 将数据转换为 numpy 数组并确保为数值类型\n",
    "real_values = pd.to_numeric(real_data_2024['收盘'], errors='coerce').values\n",
    "synth_values = pd.to_numeric(synth_data_2024['收盘'], errors='coerce').values\n",
    "\n",
    "# 检查是否存在 NaN 值\n",
    "print(\"real_values 中 NaN 的数量:\", np.isnan(real_values).sum())\n",
    "print(\"synth_values 中 NaN 的数量:\", np.isnan(synth_values).sum())\n",
    "\n",
    "# 如果存在 NaN 值，替换为零或进行插值\n",
    "if np.isnan(real_values).any():\n",
    "    real_values = np.nan_to_num(real_values)\n",
    "if np.isnan(synth_values).any():\n",
    "    synth_values = np.nan_to_num(synth_values)\n",
    "\n",
    "# 计算统计指标\n",
    "real_mse = mean_squared_error(real_values[:-1], real_values[1:])\n",
    "synth_mse = mean_squared_error(real_values, synth_values)\n",
    "\n",
    "print(f\"真实数据基准MSE: {real_mse:.4f}\")\n",
    "print(f\"合成数据MSE: {synth_mse:.4f}\")\n",
    "\n",
    "if synth_mse < real_mse:\n",
    "    improvement = (real_mse - synth_mse) / real_mse * 100\n",
    "    print(f\"误差降低 {improvement:.1f}%，模型验证成功！\")\n",
    "else:\n",
    "    print(\"需要进一步优化模型参数或结构\")\n",
    "\n",
    "# 可视化结果\n",
    "plt.figure(figsize=(21, 6))\n",
    "plt.plot(real_data_2024.index, real_values, label='真实数据', alpha=0.7)\n",
    "plt.plot(synth_data_2024.index, synth_values, label='合成数据', alpha=0.7)\n",
    "\n",
    "plt.xlabel('日期')\n",
    "plt.ylabel('收盘价')\n",
    "plt.title('')\n",
    "plt.legend()\n",
    "\n",
    "# 添加统计信息到图表\n",
    "#plt.text(0.05, 0.9, f'真实数据基准MSE: {real_mse:.4f}', transform=plt.gca().transAxes, fontsize=10)\n",
    "#plt.text(0.05, 0.85, f'合成数据MSE: {synth_mse:.4f}', transform=plt.gca().transAxes, fontsize=10)\n",
    "#plt.text(0.05, 0.8, ('模型验证成功！误差降低 {:.1f}%'.format(improvement) if synth_mse < real_mse else '需要进一步优化模型'), \n",
    "         #transform=plt.gca().transAxes, fontsize=10)\n",
    "\n",
    "# 保存可视化结果到当前文件夹\n",
    "plt.savefig('真实数据、合成数据对比.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
